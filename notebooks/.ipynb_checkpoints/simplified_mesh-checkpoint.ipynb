{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gdist\n",
    "from __future__ import division\n",
    "import scipy.spatial as spatial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "'''\n",
    "Functions to read and write vtk files\n",
    "------------------------------------\n",
    "* read takes vtk file and returns vertex and face array\n",
    "* write takes vertex and faces array and optional comment and returns vtk file\n",
    "* reading and writing of texture not supported yet\n",
    "\n",
    "TO DO: add reading comments\n",
    "'''\n",
    "\n",
    "def read_vtk(file):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    # read full file while dropping empty lines \n",
    "    vtk_df=pd.read_csv(file, header=None)\n",
    "    vtk_df=vtk_df.dropna()\n",
    "    # extract number of vertices and faces\n",
    "    number_vertices=int(vtk_df[vtk_df[0].str.contains('POINTS')][0].iloc[0].split()[1])\n",
    "    number_faces=int(vtk_df[vtk_df[0].str.contains('POLYGONS')][0].iloc[0].split()[1])\n",
    "    # read vertices into df and array\n",
    "    start_vertices= (vtk_df[vtk_df[0].str.contains('POINTS')].index.tolist()[0])+1\n",
    "    vertex_df=pd.read_csv(file, skiprows=range(start_vertices), nrows=number_vertices, sep='\\s*', header=None)\n",
    "    if np.array(vertex_df).shape[1]==3:\n",
    "        vertex_array=np.array(vertex_df)\n",
    "    # sometimes the vtk format is weird with 9 indices per line, then it has to be reshaped\n",
    "    elif np.array(vertex_df).shape[1]==9:\n",
    "        vertex_df=pd.read_csv(file, skiprows=range(start_vertices), nrows=int(number_vertices/3+1), sep='\\s*', header=None)\n",
    "        vertex_array=np.array(vertex_df.iloc[0:1,0:3])\n",
    "        vertex_array=np.append(vertex_array, vertex_df.iloc[0:1,3:6], axis=0)\n",
    "        vertex_array=np.append(vertex_array, vertex_df.iloc[0:1,6:9], axis=0)\n",
    "        for row in range(1,int(number_vertices/3+1)):\n",
    "            for col in [0,3,6]:\n",
    "                vertex_array=np.append(vertex_array, array(vertex_df.iloc[row:(row+1),col:(col+3)]),axis=0) \n",
    "        # strip rows containing nans\n",
    "        vertex_array=vertex_array[ ~np.isnan(vertex_array) ].reshape(number_vertices,3)\n",
    "    else:\n",
    "        print \"vertex indices out of shape\"\n",
    "    # read faces into df and array\n",
    "    start_faces= (vtk_df[vtk_df[0].str.contains('POLYGONS')].index.tolist()[0])+1\n",
    "    face_df=pd.read_csv(file, skiprows=range(start_faces), nrows=number_faces, sep='\\s*', header=None)\n",
    "    face_array=np.array(face_df.iloc[:,1:4])\n",
    "    # read data into df and array if exists\n",
    "    if vtk_df[vtk_df[0].str.contains('POINT_DATA')].index.tolist()!=[]:\n",
    "        start_data=(vtk_df[vtk_df[0].str.contains('POINT_DATA')].index.tolist()[0])+3\n",
    "        number_data = number_vertices\n",
    "        data_df=pd.read_csv(file, skiprows=range(start_data), nrows=number_data, sep='\\s*', header=None)\n",
    "        data_array=np.array(data_df)\n",
    "    else:\n",
    "        data_array = np.empty(0)\n",
    "    \n",
    "    return vertex_array, face_array, data_array\n",
    "\n",
    "\n",
    "\n",
    "def write_vtk(filename, vertices, faces, data=None, comment=None):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    # infer number of vertices and faces\n",
    "    number_vertices=vertices.shape[0]\n",
    "    number_faces=faces.shape[0]\n",
    "    if data is not None:\n",
    "        number_data=data.shape[0]\n",
    "    # make header and subheader dataframe\n",
    "    header=['# vtk DataFile Version 3.0',\n",
    "            '%s'%comment,\n",
    "            'ASCII',\n",
    "            'DATASET POLYDATA',\n",
    "            'POINTS %i float'%number_vertices\n",
    "             ]\n",
    "    header_df=pd.DataFrame(header)\n",
    "    sub_header=['POLYGONS %i %i'%(number_faces, 4*number_faces)]\n",
    "    sub_header_df=pd.DataFrame(sub_header)    \n",
    "    # make dataframe from vertices\n",
    "    vertex_df=pd.DataFrame(vertices)\n",
    "    # make dataframe from faces, appending first row of 3's (indicating the polygons are triangles)\n",
    "    triangles=np.reshape(3*(np.ones(number_faces)), (number_faces,1))\n",
    "    triangles=triangles.astype(int)\n",
    "    faces=faces.astype(int)\n",
    "    faces_df=pd.DataFrame(np.concatenate((triangles,faces),axis=1))\n",
    "    # write dfs to csv\n",
    "    header_df.to_csv(filename, header=None, index=False)\n",
    "    with open(filename, 'a') as f:\n",
    "        vertex_df.to_csv(f, header=False, index=False, float_format='%.3f', sep=' ')\n",
    "    with open(filename, 'a') as f:\n",
    "        sub_header_df.to_csv(f, header=False, index=False)\n",
    "    with open(filename, 'a') as f:\n",
    "        faces_df.to_csv(f, header=False, index=False, float_format='%.0f', sep=' ')\n",
    "    # if there is data append second subheader and data\n",
    "    if data!=None:\n",
    "        datapoints=data.shape[1]\n",
    "        sub_header2=['POINT_DATA %i'%(number_data),\n",
    "                     'SCALARS EmbedVertex float %i'%(datapoints),\n",
    "                     'LOOKUP_TABLE default']\n",
    "        sub_header_df2=pd.DataFrame(sub_header2)\n",
    "        data_df=pd.DataFrame(data)\n",
    "        with open(filename, 'a') as f:\n",
    "            sub_header_df2.to_csv(f, header=False, index=False)\n",
    "        with open(filename, 'a') as f:\n",
    "            data_df.to_csv(f, header=False, index=False, float_format='%.16f', sep=' ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "v,f,d = read_vtk('/scr/ilz3/myelinconnect/struct/surf_lh/orig/mid_surface/BP4T_lh_mid.vtk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sv, sf, sd = read_vtk('/scr/ilz3/myelinconnect/groupavg/indv_space/BP4T/lowres_lh_d_def.vtk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sv = sv[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for each vertex in the simplified mesh, find the closest vertex in the highres mesh\n",
    "mapping = []\n",
    "for lowres_idx in range(sv.shape[0]):\n",
    "    dist = 10000\n",
    "    closest = None\n",
    "    for highres_idx in range(v.shape[0]):\n",
    "        d = np.sqrt((v[highres_idx][0] - sv[lowres_idx][0]) ** 2 \n",
    "                  + (v[highres_idx][1] - sv[lowres_idx][1]) ** 2 \n",
    "                  + (v[highres_idx][2] - sv[lowres_idx][2]) ** 2)\n",
    "        if d < dist:\n",
    "            dist = d\n",
    "            closest = highres_idx\n",
    "    mapping.append([lowres_idx, closest])\n",
    "mapping = np.asarray(mapping) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 126.39948  117.50543  139.71094]\n",
      "[ 126.4      117.6      138.76149]\n",
      "[ 127.66485  117.84246  139.7552 ]\n",
      "[ 127.6      117.99999  138.87289]\n",
      "[ 127.56905  116.22385  139.78674]\n",
      "[ 127.6      116.39999  138.76111]\n",
      "[ 125.91573  122.31351  138.79105]\n",
      "[ 126.       122.39999  138.05463]\n",
      "[ 127.56006  120.66943  139.09203]\n",
      "[ 127.6      120.39999  138.72423]\n",
      "[ 122.30099  120.11935  139.84627]\n",
      "[ 122.       119.99999  138.89955]\n",
      "[ 129.73494  119.75185  139.3521 ]\n",
      "[ 129.60001  119.6      139.42366]\n",
      "[ 123.22951  117.91116  139.8903 ]\n",
      "[ 123.6      117.99999  138.43802]\n",
      "[ 125.73241  118.87785  139.65289]\n",
      "[ 125.6      118.8      138.72887]\n",
      "[ 127.03365  119.28188  139.483  ]\n",
      "[ 127.2      119.2      138.90459]\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print sv[mapping[i][0]]\n",
    "    print v[mapping[i][1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spatial.Voronoi?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for each vertex on the highres mesh find all nodes in a given radius\n",
    "radius = 5\n",
    "vertices = v.astype(np.float64)\n",
    "faces = f.astype(np.int32)\n",
    "inradius_matrix=gdist.local_gdist_matrix(vertices, faces, max_distance=radius)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for each vertex in the highres mesh find that node that corresponds to one on the lowres mesh and is closest\n",
    "for highres_idx in range(v.shape[0]):\n",
    "    inradius_matrix[v[highres_idx]].indices"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
